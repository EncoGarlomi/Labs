{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzq10xFmy1Vc",
        "outputId": "2081df33-7886-4bfa-b799-608790cf7300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/floresta.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кількість речень у навчальній вибірці: 7412\n",
            "Кількість речень у тестовій вибірці: 1854\n",
            "Розмір словника: 24708\n",
            "Створення матриць частот...\n",
            "Кількість унікальних тегів: 37\n",
            "Створення матриць ймовірностей (A та B)...\n",
            "Матриці готові.\n",
            "Оцінка точності HMM на тестовій вибірці...\n",
            "Точність HMM (Floresta PT): 0.9090\n",
            "Навчання NLTK Unigram+Bigram tagger...\n",
            "Точність NLTK BigramTagger: 0.9001\n",
            "------------------------------\n",
            "Різниця: 0.0089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1407902242.py:297: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  nltk_accuracy = t2.evaluate(test_sents[:500])\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import floresta\n",
        "\n",
        "# Завантаження даних NLTK\n",
        "nltk.download('floresta')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# --- 1. МОРФОЛОГІЧНІ ПРАВИЛА ДЛЯ ПОРТУГАЛЬСЬКОЇ МОВИ ---\n",
        "\n",
        "punct = set(string.punctuation + \"«»„“—…\")\n",
        "\n",
        "def assign_unk_pt(tok):\n",
        "    \"\"\"\n",
        "    Призначення міток для невідомих слів (Португальська мова)\n",
        "    \"\"\"\n",
        "    # Цифри\n",
        "    if any(char.isdigit() for char in tok):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Пунктуація\n",
        "    elif any(char in punct for char in tok):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Великі літери\n",
        "    elif any(char.isupper() for char in tok):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Іменники (substantivos)\n",
        "    # -ção (ação), -dade (liberdade), -mento (momento), -ismo, -ista\n",
        "    elif any(tok.endswith(suffix) for suffix in [\"cao\", \"ção\", \"dade\", \"mento\", \"ismo\", \"ista\", \"or\", \"s\"]):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Дієслова (verbos)\n",
        "    # -ar, -er, -ir, -ando, -endo, -indo, -u, -a, -e\n",
        "    elif any(tok.endswith(suffix) for suffix in [\"ar\", \"er\", \"ir\", \"ando\", \"endo\", \"indo\", \"am\", \"em\"]):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Прикметники (adjetivos)\n",
        "    # -oso, -vel, -al, -ico\n",
        "    elif any(tok.endswith(suffix) for suffix in [\"oso\", \"osa\", \"vel\", \"al\", \"ico\", \"ica\"]):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Прислівники (advérbios)\n",
        "    # -mente\n",
        "    elif any(tok.endswith(suffix) for suffix in [\"mente\"]):\n",
        "        return \"--unk_adv--\"\n",
        "\n",
        "    return \"--unk--\"\n",
        "\n",
        "# --- 2. ПІДГОТОВКА ДАНИХ (FLORESTA) ---\n",
        "\n",
        "def get_floresta_data():\n",
        "    \"\"\"\n",
        "    Завантажує корпус Floresta, спрощує теги та розділяє на train/test.\n",
        "    \"\"\"\n",
        "    # Завантажуємо речення. Floresta має теги типу 'n+art', беремо лише 'n' (першу частину)\n",
        "    # або останню частину, залежно від стратегії. Тут візьмемо останню частину (спрощено).\n",
        "    tagged_sents = []\n",
        "    for sent in floresta.tagged_sents():\n",
        "        simplified_sent = []\n",
        "        for word, tag in sent:\n",
        "            # Очищення тегу (видалення метаданих)\n",
        "            # Наприклад: \"H+n\" -> \"n\", \"art\" -> \"art\"\n",
        "            if '+' in tag:\n",
        "                clean_tag = tag.split('+')[-1] # Беремо частину після +\n",
        "            else:\n",
        "                clean_tag = tag\n",
        "            simplified_sent.append((word.lower(), clean_tag)) # Lowercase для слів\n",
        "        tagged_sents.append(simplified_sent)\n",
        "\n",
        "    # Розділення 80/20\n",
        "    split = int(len(tagged_sents) * 0.8)\n",
        "    train_data = tagged_sents[:split]\n",
        "    test_data = tagged_sents[split:]\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "# Отримуємо дані\n",
        "train_sents, test_sents = get_floresta_data()\n",
        "print(f\"Кількість речень у навчальній вибірці: {len(train_sents)}\")\n",
        "print(f\"Кількість речень у тестовій вибірці: {len(test_sents)}\")\n",
        "\n",
        "# Створення словника (Vocabulary)\n",
        "vocab = {}\n",
        "vocab[\"--n--\"] = 0 # Спеціальний токен кінця речення\n",
        "vocab[\"--unk--\"] = 1 # Базовий невідомий\n",
        "# Додаємо спецтокени з assign_unk_pt\n",
        "special_tokens = [\"--unk_digit--\", \"--unk_punct--\", \"--unk_upper--\",\n",
        "                  \"--unk_noun--\", \"--unk_verb--\", \"--unk_adj--\", \"--unk_adv--\"]\n",
        "for t in special_tokens:\n",
        "    vocab[t] = len(vocab)\n",
        "\n",
        "# Заповнюємо словник словами з тренувальної вибірки\n",
        "# Ми фільтруємо слова, що зустрічаються < 2 разів, щоб модель вчилася працювати з UNK,\n",
        "# але для спрощення лабораторної додамо всі слова.\n",
        "for sent in train_sents:\n",
        "    for word, tag in sent:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "print(f\"Розмір словника: {len(vocab)}\")\n",
        "\n",
        "# --- 3. СТВОРЕННЯ МАТРИЦЬ HMM ---\n",
        "\n",
        "def create_dictionaries(training_corpus, vocab):\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "\n",
        "    prev_tag = '--s--' # Start tag\n",
        "\n",
        "    for sent in training_corpus:\n",
        "        prev_tag = '--s--'\n",
        "        for word, tag in sent:\n",
        "            transition_counts[(prev_tag, tag)] += 1\n",
        "\n",
        "            # Якщо слово є в словнику, використовуємо його, інакше - UNK логіку\n",
        "            if word in vocab:\n",
        "                emission_counts[(tag, word)] += 1\n",
        "            else:\n",
        "                unk_token = assign_unk_pt(word)\n",
        "                emission_counts[(tag, unk_token)] += 1\n",
        "\n",
        "            tag_counts[tag] += 1\n",
        "            prev_tag = tag\n",
        "\n",
        "    return emission_counts, transition_counts, tag_counts\n",
        "\n",
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(all_tags)\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        for j in range(num_tags):\n",
        "            count = transition_counts.get((all_tags[i], all_tags[j]), 0)\n",
        "            count_prev = tag_counts[all_tags[i]]\n",
        "            A[i, j] = (count + alpha) / (count_prev + alpha * num_tags)\n",
        "    return A, all_tags\n",
        "\n",
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab_list):\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(all_tags)\n",
        "    num_words = len(vocab_list)\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        tag = all_tags[i]\n",
        "        count_tag = tag_counts[tag]\n",
        "\n",
        "        # Для оптимізації пройдемося тільки по словах, які цей тег емітував\n",
        "        # Але для повної матриці треба заповнити все.\n",
        "        # Це може бути повільно для великого словника.\n",
        "        # Тому тут зробимо ітерацію по словнику.\n",
        "\n",
        "        for j, word in enumerate(vocab_list):\n",
        "            count = emission_counts.get((tag, word), 0)\n",
        "            B[i, j] = (count + alpha) / (count_tag + alpha * num_words)\n",
        "\n",
        "    return B\n",
        "\n",
        "# Генерація матриць\n",
        "print(\"Створення матриць частот...\")\n",
        "emission_counts, transition_counts, tag_counts = create_dictionaries(train_sents, vocab)\n",
        "states = sorted(tag_counts.keys())\n",
        "print(f\"Кількість унікальних тегів: {len(states)}\")\n",
        "\n",
        "print(\"Створення матриць ймовірностей (A та B)...\")\n",
        "alpha = 0.001\n",
        "vocab_list = list(vocab.keys())\n",
        "# Оновлюємо vocab, щоб порядок відповідав vocab_list (для індексації у Viterbi)\n",
        "vocab_map = {word: i for i, word in enumerate(vocab_list)}\n",
        "\n",
        "A, _ = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, vocab_list)\n",
        "print(\"Матриці готові.\")\n",
        "\n",
        "# --- 4. АЛГОРИТМ ВІТЕРБІ ---\n",
        "\n",
        "def initialize_viterbi(states, tag_counts, A, B, sent, vocab_map):\n",
        "    num_tags = len(states)\n",
        "    best_probs = np.zeros((num_tags, len(sent)))\n",
        "    best_paths = np.zeros((num_tags, len(sent)), dtype=int)\n",
        "\n",
        "    # Початковий стан --s--. У нас його немає в матрицях явно як рядка\n",
        "    # (бо ми брали keys з tag_counts), але ми маємо переходи з '--s--' у transition_counts.\n",
        "    # Для спрощення припустимо рівномірний старт або використаємо переходи з '--s--'.\n",
        "\n",
        "    # Обчислимо початкові ймовірності (Start Probabilities)\n",
        "    # P(tag | start)\n",
        "    start_probs = np.zeros(num_tags)\n",
        "    total_starts = sum(transition_counts[('--s--', t)] for t in states)\n",
        "    for i, tag in enumerate(states):\n",
        "        count = transition_counts[('--s--', tag)]\n",
        "        start_probs[i] = (count + alpha) / (total_starts + alpha * num_tags)\n",
        "\n",
        "    first_word = sent[0][0] # sent is list of (word, tag), need word\n",
        "\n",
        "    # Обробка першого слова\n",
        "    if first_word in vocab_map:\n",
        "        word_idx = vocab_map[first_word]\n",
        "    else:\n",
        "        word_idx = vocab_map[assign_unk_pt(first_word)]\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        # log(P(start->tag)) + log(P(tag emits word))\n",
        "        best_probs[i, 0] = math.log(start_probs[i]) + math.log(B[i, word_idx])\n",
        "\n",
        "    return best_probs, best_paths\n",
        "\n",
        "def viterbi_forward(A, B, sent, best_probs, best_paths, vocab_map):\n",
        "    num_tags = best_probs.shape[0]\n",
        "\n",
        "    for i in range(1, len(sent)):\n",
        "        word = sent[i][0]\n",
        "        if word in vocab_map:\n",
        "            word_idx = vocab_map[word]\n",
        "        else:\n",
        "            word_idx = vocab_map[assign_unk_pt(word)]\n",
        "\n",
        "        for j in range(num_tags): # Поточний тег\n",
        "            best_prob_i = float(\"-inf\")\n",
        "            best_path_i = None\n",
        "\n",
        "            for k in range(num_tags): # Попередній тег\n",
        "                # prob = prev_prob + trans_prob + emiss_prob\n",
        "                prob = best_probs[k, i-1] + math.log(A[k, j]) + math.log(B[j, word_idx])\n",
        "\n",
        "                if prob > best_prob_i:\n",
        "                    best_prob_i = prob\n",
        "                    best_path_i = k\n",
        "\n",
        "            best_probs[j, i] = best_prob_i\n",
        "            best_paths[j, i] = best_path_i\n",
        "\n",
        "    return best_probs, best_paths\n",
        "\n",
        "def viterbi_backward(best_probs, best_paths, states):\n",
        "    m = best_probs.shape[1] # length of sentence\n",
        "    z = [None] * m\n",
        "    pred = [None] * m\n",
        "\n",
        "    best_prob_last = float('-inf')\n",
        "    for k in range(len(states)):\n",
        "        if best_probs[k, m-1] > best_prob_last:\n",
        "            best_prob_last = best_probs[k, m-1]\n",
        "            z[m-1] = k\n",
        "\n",
        "    pred[m-1] = states[z[m-1]]\n",
        "\n",
        "    for i in range(m-2, -1, -1):\n",
        "        z[i] = best_paths[z[i+1], i+1]\n",
        "        pred[i] = states[z[i]]\n",
        "\n",
        "    return pred\n",
        "\n",
        "# --- 5. ОЦІНКА ТОЧНОСТІ ---\n",
        "\n",
        "def evaluate_hmm(test_data, states, tag_counts, A, B, vocab_map):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sent in test_data: # Беремо перші 100 для швидкості, якщо повільно\n",
        "        if len(sent) == 0: continue\n",
        "\n",
        "        # Viterbi\n",
        "        best_probs, best_paths = initialize_viterbi(states, tag_counts, A, B, sent, vocab_map)\n",
        "        best_probs, best_paths = viterbi_forward(A, B, sent, best_probs, best_paths, vocab_map)\n",
        "        pred_tags = viterbi_backward(best_probs, best_paths, states)\n",
        "\n",
        "        # Comparison\n",
        "        for (word, true_tag), pred_tag in zip(sent, pred_tags):\n",
        "            if true_tag == pred_tag:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "print(\"Оцінка точності HMM на тестовій вибірці...\")\n",
        "# Тестуємо на підмножині для швидкості (або прибрати зріз [:500] для повного тесту)\n",
        "hmm_accuracy = evaluate_hmm(test_sents[:500], states, tag_counts, A, B, vocab_map)\n",
        "print(f\"Точність HMM (Floresta PT): {hmm_accuracy:.4f}\")\n",
        "\n",
        "# --- 6. ПОРІВНЯННЯ З NLTK ---\n",
        "\n",
        "print(\"Навчання NLTK Unigram+Bigram tagger...\")\n",
        "# NLTK потребує дані у форматі list of lists of tuples, що ми вже маємо (train_sents)\n",
        "t0 = nltk.DefaultTagger('n') # За замовчуванням іменник\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
        "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
        "\n",
        "nltk_accuracy = t2.evaluate(test_sents[:500])\n",
        "print(f\"Точність NLTK BigramTagger: {nltk_accuracy:.4f}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Різниця: {hmm_accuracy - nltk_accuracy:.4f}\")"
      ]
    }
  ]
}