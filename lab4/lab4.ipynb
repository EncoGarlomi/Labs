{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFfy21DkrFQc",
        "outputId": "d5e6dd34-f77a-477d-ee47-19af02d1d4df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocs_8kEWqVyC",
        "outputId": "08002d1d-c8b8-4ba4-997d-243fa03a01ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—ñ–≤ —Å–ª—ñ–≤ (—Ü–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ 1-2 —Ö–≤–∏–ª–∏–Ω–∏)...\n",
            "--2025-12-03 14:51:26--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.150.65, 108.157.150.95, 108.157.150.62, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.150.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‚Äòcc.en.300.vec.gz‚Äô\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G  49.7MB/s    in 2m 7s   \n",
            "\n",
            "2025-12-03 14:53:35 (9.93 MB/s) - ‚Äòcc.en.300.vec.gz‚Äô saved [1325960915/1325960915]\n",
            "\n",
            "--2025-12-03 14:55:01--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.cs.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.150.55, 108.157.150.95, 108.157.150.65, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.150.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1262989069 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‚Äòcc.cs.300.vec.gz‚Äô\n",
            "\n",
            "cc.cs.300.vec.gz    100%[===================>]   1.18G  94.0MB/s    in 89s     \n",
            "\n",
            "2025-12-03 14:56:31 (13.5 MB/s) - ‚Äòcc.cs.300.vec.gz‚Äô saved [1262989069/1262989069]\n",
            "\n",
            "–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ EN —Å–ª—ñ–≤: 50000\n",
            "–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ CS —Å–ª—ñ–≤: 50000\n",
            ">>> –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–ª–æ–≤–Ω–∏–∫–∞ –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤...\n",
            "File ‚Äòen-cs.txt‚Äô already there; not retrieving.\n",
            "\n",
            "–í—Å—å–æ–≥–æ –ø–∞—Ä —É —Å–ª–æ–≤–Ω–∏–∫—É: 52552\n",
            "–†–æ–∑–º—ñ—Ä Train —Å–ª–æ–≤–Ω–∏–∫–∞: 5000\n",
            "–†–æ–∑–º—ñ—Ä Test —Å–ª–æ–≤–Ω–∏–∫–∞: 1500\n"
          ]
        }
      ],
      "source": [
        "# --- 0. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–∞ –Ü–º–ø–æ—Ä—Ç–∏ ---\n",
        "!pip install gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import KeyedVectors\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—ñ–≤ (FastText) ---\n",
        "print(\">>> –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—ñ–≤ —Å–ª—ñ–≤ (—Ü–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ 1-2 —Ö–≤–∏–ª–∏–Ω–∏)...\")\n",
        "\n",
        "# –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ (Lang 1)\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gzip -d -f cc.en.300.vec.gz\n",
        "# limit=50000 –µ–∫–æ–Ω–æ–º–∏—Ç—å –ø–∞–º'—è—Ç—å. –î–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ –º–æ–∂–Ω–∞ –∑–±—ñ–ª—å—à–∏—Ç–∏ –¥–æ 100000, —è–∫—â–æ Colab –¥–æ–∑–≤–æ–ª—è—î\n",
        "lang1_embeddings = KeyedVectors.load_word2vec_format(\"cc.en.300.vec\", binary=False, limit=50000)\n",
        "\n",
        "# –ß–µ—Å—å–∫–∞ (Lang 2)\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.cs.300.vec.gz\n",
        "!gzip -d -f cc.cs.300.vec.gz\n",
        "lang2_embeddings = KeyedVectors.load_word2vec_format(\"cc.cs.300.vec\", binary=False, limit=50000)\n",
        "\n",
        "print(f\"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ EN —Å–ª—ñ–≤: {len(lang1_embeddings)}\")\n",
        "print(f\"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ CS —Å–ª—ñ–≤: {len(lang2_embeddings)}\")\n",
        "\n",
        "# --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–ª–æ–≤–Ω–∏–∫–∞ MUSE (EN-CS) ---\n",
        "print(\">>> –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–ª–æ–≤–Ω–∏–∫–∞ –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤...\")\n",
        "!wget -nc https://dl.fbaipublicfiles.com/arrival/dictionaries/en-cs.txt\n",
        "\n",
        "def load_dict(filename):\n",
        "    # –ß–∏—Ç–∞—î–º–æ —Ñ–∞–π–ª. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ on_bad_lines='skip' –¥–ª—è –Ω–æ–≤–∏—Ö –≤–µ—Ä—Å—ñ–π pandas\n",
        "    # –°–ª–æ–≤–Ω–∏–∫–∏ MUSE –∑–∞–∑–≤–∏—á–∞–π —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ —Ç–∞–±—É–ª—è—Ü—ñ—î—é (\\t) –∞–±–æ –ø—Ä–æ–±—ñ–ª–æ–º\n",
        "    try:\n",
        "        df = pd.read_csv(filename, sep='\\t', header=None, names=['en', 'cs'], on_bad_lines='skip')\n",
        "    except TypeError:\n",
        "        # –§–æ–ª–±–µ–∫ –¥–ª—è —Å—Ç–∞—Ä–∏—Ö –≤–µ—Ä—Å—ñ–π pandas\n",
        "        df = pd.read_csv(filename, sep='\\t', header=None, names=['en', 'cs'], error_bad_lines=False)\n",
        "\n",
        "    # –Ø–∫—â–æ –∑—á–∏—Ç–∞–ª–æ—Å—å –≤ –æ–¥–Ω—É –∫–æ–ª–æ–Ω–∫—É, –∑–Ω–∞—á–∏—Ç—å —Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫ - –ø—Ä–æ–±—ñ–ª\n",
        "    if df.shape[1] < 2:\n",
        "        try:\n",
        "            df = pd.read_csv(filename, sep=' ', header=None, names=['en', 'cs'], on_bad_lines='skip')\n",
        "        except TypeError:\n",
        "            df = pd.read_csv(filename, sep=' ', header=None, names=['en', 'cs'], error_bad_lines=False)\n",
        "\n",
        "    # –í–∏–¥–∞–ª—è—î–º–æ –ø—Ä–æ–ø—É—Å–∫–∏ —Ç–∞ –¥—É–±–ª—ñ–∫–∞—Ç–∏\n",
        "    df = df.dropna().drop_duplicates()\n",
        "\n",
        "    # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ª–æ–≤–Ω–∏–∫ {–∞–Ω–≥–ª—ñ–π—Å—å–∫–µ_—Å–ª–æ–≤–æ: —á–µ—Å—å–∫–µ_—Å–ª–æ–≤–æ}\n",
        "    return dict(zip(df['en'], df['cs']))\n",
        "\n",
        "full_dict = load_dict('en-cs.txt')\n",
        "print(f\"–í—Å—å–æ–≥–æ –ø–∞—Ä —É —Å–ª–æ–≤–Ω–∏–∫—É: {len(full_dict)}\")\n",
        "\n",
        "# –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ Train (–Ω–∞–≤—á–∞–Ω–Ω—è) —Ç–∞ Test (–ø–µ—Ä–µ–≤—ñ—Ä–∫–∞)\n",
        "all_keys = list(full_dict.keys())\n",
        "train_keys = all_keys[:5000]\n",
        "test_keys = all_keys[5000:6500]\n",
        "\n",
        "l1_l2_train = {k: full_dict[k] for k in train_keys if k in full_dict}\n",
        "l1_l2_test = {k: full_dict[k] for k in test_keys if k in full_dict}\n",
        "\n",
        "print(f\"–†–æ–∑–º—ñ—Ä Train —Å–ª–æ–≤–Ω–∏–∫–∞: {len(l1_l2_train)}\")\n",
        "print(f\"–†–æ–∑–º—ñ—Ä Test —Å–ª–æ–≤–Ω–∏–∫–∞: {len(l1_l2_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—å X —Ç–∞ Y ---\n",
        "def get_matrices(l1_l2, l2_embeddings, l1_embeddings):\n",
        "    X_l = []\n",
        "    Y_l = []\n",
        "    for l1_word, l2_word in l1_l2.items():\n",
        "        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î —Å–ª–æ–≤–∞ —É –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–∞—Ö (FastText)\n",
        "        if l1_word in l1_embeddings and l2_word in l2_embeddings:\n",
        "            X_l.append(l1_embeddings[l1_word])\n",
        "            Y_l.append(l2_embeddings[l2_word])\n",
        "    return np.array(X_l), np.array(Y_l)\n",
        "\n",
        "X_train, Y_train = get_matrices(l1_l2_train, lang2_embeddings, lang1_embeddings)\n",
        "print(f\"–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—å –Ω–∞–≤—á–∞–Ω–Ω—è: X={X_train.shape}, Y={Y_train.shape}\")\n",
        "\n",
        "# --- 4. –ì—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ ---\n",
        "def compute_loss(X, Y, R):\n",
        "    m = X.shape[0]\n",
        "    diff = np.dot(X, R) - Y\n",
        "    loss = np.sum(diff**2) / m\n",
        "    return loss\n",
        "\n",
        "def compute_gradient(X, Y, R):\n",
        "    m = X.shape[0]\n",
        "    gradient = np.dot(X.T, np.dot(X, R) - Y) * (2 / m)\n",
        "    return gradient\n",
        "\n",
        "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
        "    np.random.seed(129)\n",
        "    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–∏–ø–∞–¥–∫–æ–≤–æ—é –º–∞—Ç—Ä–∏—Ü–µ—é\n",
        "    R = np.random.rand(X.shape[1], X.shape[1])\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"–Ü—Ç–µ—Ä–∞—Ü—ñ—è {i}: Loss = {compute_loss(X, Y, R):.4f}\")\n",
        "        gradient = compute_gradient(X, Y, R)\n",
        "        R -= learning_rate * gradient\n",
        "    return R\n",
        "\n",
        "print(\">>> –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è...\")\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–∂–Ω–∞ –∑–º—ñ–Ω—é–≤–∞—Ç–∏. learning_rate=0.8 –¥–æ–±—Ä–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤\n",
        "R_train = align_embeddings(X_train, Y_train, train_steps=500, learning_rate=0.8)\n",
        "print(\"–ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\")\n",
        "\n",
        "# --- 5. –§—É–Ω–∫—Ü—ñ—è –ø–µ—Ä–µ–∫–ª–∞–¥—É —Ç–∞ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ ---\n",
        "def translate(word, R, l1_embeddings, l2_embeddings, k=1):\n",
        "    if word not in l1_embeddings:\n",
        "        return None\n",
        "    # 1. –í–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞\n",
        "    v = l1_embeddings[word]\n",
        "    # 2. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è\n",
        "    v_prime = np.dot(v, R)\n",
        "    # 3. –ü–æ—à—É–∫ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ\n",
        "    sims = l2_embeddings.cosine_similarities(v_prime, l2_embeddings.vectors)\n",
        "    best_idxs = np.argsort(sims)[-k:][::-1]\n",
        "    return [l2_embeddings.index_to_key[i] for i in best_idxs]\n",
        "\n",
        "# –¢–µ—Å—Ç\n",
        "test_word = \"cat\"\n",
        "print(f\"–ü–µ—Ä–µ–∫–ª–∞–¥ '{test_word}': {translate(test_word, R_train, lang1_embeddings, lang2_embeddings, k=3)}\")\n",
        "\n",
        "# --- 6. –¢–æ—á–Ω—ñ—Å—Ç—å (Accuracy) ---\n",
        "def calculate_accuracy(l1_l2_test, R, l1_emb, l2_emb):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # –ë–µ—Ä–µ–º–æ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è\n",
        "    test_items = list(l1_l2_test.items())[:500]\n",
        "\n",
        "    for l1_word, l2_word in test_items:\n",
        "        if l1_word in l1_emb and l2_word in l2_emb:\n",
        "            total += 1\n",
        "            pred = translate(l1_word, R, l1_emb, l2_emb, k=1)\n",
        "            if pred and pred[0] == l2_word:\n",
        "                correct += 1\n",
        "\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "acc = calculate_accuracy(l1_l2_test, R_train, lang1_embeddings, lang2_embeddings)\n",
        "print(f\"–¢–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ: {acc:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytqO29dpqwv3",
        "outputId": "3e362ebe-dac9-466e-9b09-3cf811dbcea0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—å –Ω–∞–≤—á–∞–Ω–Ω—è: X=(3717, 300), Y=(3717, 300)\n",
            ">>> –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è...\n",
            "–Ü—Ç–µ—Ä–∞—Ü—ñ—è 0: Loss = 102.3514\n",
            "–Ü—Ç–µ—Ä–∞—Ü—ñ—è 100: Loss = 25.9041\n",
            "–Ü—Ç–µ—Ä–∞—Ü—ñ—è 200: Loss = 11.7877\n",
            "–Ü—Ç–µ—Ä–∞—Ü—ñ—è 300: Loss = 6.6362\n",
            "–Ü—Ç–µ—Ä–∞—Ü—ñ—è 400: Loss = 4.2729\n",
            "–ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\n",
            "–ü–µ—Ä–µ–∫–ª–∞–¥ 'cat': ['pes', 'kocour', 'pejsek']\n",
            "–¢–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ: 10.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7 & 8. –û–±—Ä–æ–±–∫–∞ —Ç–≤—ñ—Ç—ñ–≤ —Ç–∞ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è ---\n",
        "def process_tweet(tweet):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    return [stemmer.stem(w) for w in tokens if w not in stopwords_english and w not in string.punctuation]\n",
        "\n",
        "def get_tweet_vector(tweet, model):\n",
        "    tokens = process_tweet(tweet)\n",
        "    vectors = [model[w] for w in tokens if w in model]\n",
        "    if not vectors:\n",
        "        return np.zeros(300)\n",
        "    return np.mean(vectors, axis=0) # –£—Å–µ—Ä–µ–¥–Ω–µ–Ω–∏–π –≤–µ–∫—Ç–æ—Ä\n",
        "\n",
        "# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–≤—ñ—Ç–∏\n",
        "all_tweets = twitter_samples.strings('positive_tweets.json') + twitter_samples.strings('negative_tweets.json')\n",
        "# –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à—ñ 2000 –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó (–º–æ–∂–Ω–∞ –≤–∑—è—Ç–∏ –≤—Å—ñ)\n",
        "all_tweets = all_tweets[:2000]\n",
        "\n",
        "print(\">>> –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è —Ç–≤—ñ—Ç—ñ–≤...\")\n",
        "tweet_vectors_dict = {i: get_tweet_vector(t, lang1_embeddings) for i, t in enumerate(all_tweets)}\n",
        "\n",
        "# --- 9. LSH (Locality Sensitive Hashing) ---\n",
        "def hash_func(vec, planes):\n",
        "    # –°–∫–∞–ª—è—Ä–Ω–∏–π –¥–æ–±—É—Ç–æ–∫ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –ø–ª–æ—â–∏–Ω–∏. –ó–Ω–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –¥–∞—î –±—ñ—Ç —Ö–µ—à—É.\n",
        "    dot_product = np.dot(vec, planes)\n",
        "    sign = (dot_product >= 0).astype(int)\n",
        "    hash_val = 0\n",
        "    for i, s in enumerate(sign):\n",
        "        hash_val += s * (2**i)\n",
        "    return hash_val\n",
        "\n",
        "def make_hash_table(vecs_dict, planes):\n",
        "    table = {}\n",
        "    for idx, vec in vecs_dict.items():\n",
        "        h = hash_func(vec, planes)\n",
        "        table.setdefault(h, []).append(idx)\n",
        "    return table\n",
        "\n",
        "def create_lsh(vecs_dict, n_planes=10, n_tables=5):\n",
        "    tables = []\n",
        "    planes_list = []\n",
        "    dim = list(vecs_dict.values())[0].shape[0]\n",
        "\n",
        "    for _ in range(n_tables):\n",
        "        planes = np.random.normal(size=(dim, n_planes))\n",
        "        planes_list.append(planes)\n",
        "        tables.append(make_hash_table(vecs_dict, planes))\n",
        "\n",
        "    return tables, planes_list\n",
        "\n",
        "print(\">>> –°—Ç–≤–æ—Ä–µ–Ω–Ω—è LSH —Ç–∞–±–ª–∏—Ü—å...\")\n",
        "tables, planes_list = create_lsh(tweet_vectors_dict, n_planes=10, n_tables=5)\n",
        "\n",
        "# --- 10 & 11. –ü–æ—à—É–∫ —Å—É—Å—ñ–¥—ñ–≤ ---\n",
        "def lsh_knn(doc_id, vecs_dict, tables, planes_list, k=5):\n",
        "    query_vec = vecs_dict[doc_id]\n",
        "    candidates = set()\n",
        "\n",
        "    # –ó–±–∏—Ä–∞—î–º–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –∑ —É—Å—ñ—Ö —Ç–∞–±–ª–∏—Ü—å\n",
        "    for table, planes in zip(tables, planes_list):\n",
        "        h = hash_func(query_vec, planes)\n",
        "        if h in table:\n",
        "            candidates.update(table[h])\n",
        "\n",
        "    if doc_id in candidates: candidates.remove(doc_id)\n",
        "\n",
        "    # –°–æ—Ä—Ç—É—î–º–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –∑–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ—é –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—é\n",
        "    cand_list = list(candidates)\n",
        "    if not cand_list: return []\n",
        "\n",
        "    scores = []\n",
        "    for cand_id in cand_list:\n",
        "        cand_vec = vecs_dict[cand_id]\n",
        "        cos_sim = np.dot(query_vec, cand_vec) / (np.linalg.norm(query_vec)*np.linalg.norm(cand_vec) + 1e-9)\n",
        "        scores.append((cand_id, cos_sim))\n",
        "\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scores[:k]\n",
        "\n",
        "# –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è\n",
        "query_id = 100 # –í–∏–ø–∞–¥–∫–æ–≤–∏–π —Ç–≤—ñ—Ç\n",
        "neighbors = lsh_knn(query_id, tweet_vectors_dict, tables, planes_list, k=3)\n",
        "\n",
        "print(f\"\\n–ó–ê–ü–ò–¢:\\n{all_tweets[query_id]}\")\n",
        "print(\"\\n–ó–ù–ê–ô–î–ï–ù–Ü –°–•–û–ñ–Ü:\")\n",
        "for n_id, score in neighbors:\n",
        "    print(f\"[Score {score:.2f}]: {all_tweets[n_id]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUPqdVJhrLO5",
        "outputId": "f23e113f-9199-4657-e63b-7c8e48db1a4f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è —Ç–≤—ñ—Ç—ñ–≤...\n",
            ">>> –°—Ç–≤–æ—Ä–µ–Ω–Ω—è LSH —Ç–∞–±–ª–∏—Ü—å...\n",
            "\n",
            "–ó–ê–ü–ò–¢:\n",
            "@metalgear_jp @Kojima_Hideo I want you're T-shirts ! They are so cool ! :D\n",
            "\n",
            "–ó–ù–ê–ô–î–ï–ù–Ü –°–•–û–ñ–Ü:\n",
            "[Score 0.70]: @virtuallykaren cool - thanks for thinking of me - hope all is well :-)\n",
            "[Score 0.63]: It's still so nice out right now, I üíú it! :) :) :)\n",
            "[Score 0.57]: @IronMillTech Also, what is it you want me to do with this info? I shan't be ditching @ecotricity! And I'll die before I vote Tory...:)\n"
          ]
        }
      ]
    }
  ]
}