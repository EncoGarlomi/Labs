{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsIHPyaziNJW",
        "outputId": "acd2af88-9db5-4245-9133-eee65d09560c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package sentence_polarity to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кількість тренувальних прикладів: 8528\n",
            "Кількість тестових прикладів: 2134\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import sentence_polarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "nltk.download('sentence_polarity')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_data():\n",
        "    pos_sents = [\" \".join(sent) for sent in sentence_polarity.sents(categories='pos')]\n",
        "    neg_sents = [\" \".join(sent) for sent in sentence_polarity.sents(categories='neg')]\n",
        "\n",
        "    # Розділення на тренувальну (80%) та тестову (20%) вибірки\n",
        "    # Дані у sentence_polarity вже збалансовані (5331 поз, 5331 нег)\n",
        "\n",
        "    split_pos = int(len(pos_sents) * 0.8)\n",
        "    split_neg = int(len(neg_sents) * 0.8)\n",
        "\n",
        "    train_pos = pos_sents[:split_pos]\n",
        "    test_pos = pos_sents[split_pos:]\n",
        "\n",
        "    train_neg = neg_sents[:split_neg]\n",
        "    test_neg = neg_sents[split_neg:]\n",
        "\n",
        "    train_x = train_pos + train_neg\n",
        "    test_x = test_pos + test_neg\n",
        "\n",
        "    # Створення міток (1 - позитив, 0 - негатив)\n",
        "    train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "    test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
        "\n",
        "    return train_x, test_x, train_y, test_y\n",
        "\n",
        "train_x, test_x, train_y, test_y = get_data()\n",
        "print(f\"Кількість тренувальних прикладів: {len(train_x)}\")\n",
        "print(f\"Кількість тестових прикладів: {len(test_x)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(text):\n",
        "    \"\"\"\n",
        "    Функція для токенізації, стемінгу та видалення стоп-слів.\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "\n",
        "    # Видаляємо посилання, хештеги тощо (стандартна очистка)\n",
        "    text = re.sub(r'\\$\\w*', '', text)\n",
        "    text = re.sub(r'^RT[\\s]+', '', text)\n",
        "    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    clean_tokens = []\n",
        "    for word in tokens:\n",
        "        if (word not in stopwords_english and word not in string.punctuation):\n",
        "            stem_word = stemmer.stem(word)\n",
        "            clean_tokens.append(stem_word)\n",
        "\n",
        "    return clean_tokens\n",
        "\n",
        "def build_freqs(texts, ys):\n",
        "    \"\"\"\n",
        "    Створення словника частот: {(слово, мітка): кількість}\n",
        "    \"\"\"\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "    freqs = {}\n",
        "    for y, text in zip(yslist, texts):\n",
        "        for word in process_text(text):\n",
        "            pair = (word, y)\n",
        "            freqs[pair] = freqs.get(pair, 0) + 1\n",
        "    return freqs\n",
        "\n",
        "# Будуємо словник частот на основі тренувальних даних\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "print(f\"Розмір словника частот: {len(freqs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adrN10sijjjc",
        "outputId": "54eb843d-5796-4c25-e987-225dbf476ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Розмір словника частот: 18449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    # Використовуємо clip для уникнення overflow помилок при великих від'ємних z\n",
        "    z = np.clip(z, -500, 500)\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "    return h\n",
        "\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    m = x.shape[0]\n",
        "\n",
        "    for i in range(0, num_iters):\n",
        "        z = np.dot(x, theta)\n",
        "        h = sigmoid(z)\n",
        "\n",
        "        # Градієнтний крок\n",
        "        # theta = theta - (alpha / m) * X^T * (h - y)\n",
        "        theta = theta - (alpha / m) * np.dot(x.T, (h - y))\n",
        "\n",
        "        # Розрахунок втрат (для моніторингу)\n",
        "        if i % 100 == 0:\n",
        "            epsilon = 1e-15\n",
        "            h_safe = np.clip(h, epsilon, 1 - epsilon)\n",
        "            J = (-1/m) * np.sum(y * np.log(h_safe) + (1-y) * np.log(1 - h_safe))\n",
        "            # print(f\"Ітерація {i}, Втрати: {J}\") # Можна розкоментувати для налагодження\n",
        "\n",
        "    J = (-1/m) * np.sum(y * np.log(h_safe) + (1-y) * np.log(1 - h_safe))\n",
        "    return J, theta\n",
        "\n",
        "def extract_features(text, freqs):\n",
        "    word_l = process_text(text)\n",
        "    x = np.zeros((1, 3))\n",
        "    x[0,0] = 1 # Bias unit\n",
        "\n",
        "    for word in word_l:\n",
        "        # Підрахунок позитивних частот\n",
        "        x[0,1] += freqs.get((word, 1.0), 0)\n",
        "        # Підрахунок негативних частот\n",
        "        x[0,2] += freqs.get((word, 0.0), 0)\n",
        "\n",
        "    return x\n",
        "\n",
        "#НАВЧАННЯ МОДЕЛІ\n",
        "\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "# 2. Ініціалізація параметрів\n",
        "Y = train_y\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), alpha=1e-9, num_iters=10000)\n",
        "\n",
        "print(f\"Помилка після навчання: {J:.8f}\")\n",
        "print(f\"Вектор ваг (theta): {theta.reshape(-1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIxYIKiNjmit",
        "outputId": "eaf79129-8ce1-45ca-d774-b72b1c97d8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Помилка після навчання: 0.68908720\n",
            "Вектор ваг (theta): [ 2.40108218e-09  1.42376909e-04 -1.43950665e-04]\n"
          ]
        }
      ]
    }
  ]
}